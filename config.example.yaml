# Unified configuration file for SearXNG + Tavily Adapter
#
# IMPORTANT: copy this file to config.yaml and adjust values for your setup:
# cp config.example.yaml config.yaml
#
# Make sure you change:
# - server.secret_key (for SearXNG)
# - adapter.searxng_url (if you need a custom endpoint)
# - adapter.scraper.user_agent (optional)
#
# CACHING ARCHITECTURE:
# This setup uses TWO separate caching layers:
# 1. SearXNG's Redis/Valkey cache (configured in 'valkey' section below)
#    - Caches search results, rate limiting, sessions
#    - Shared across all SearXNG requests
# 2. Adapter's in-memory cache (configured in 'adapter.search' section)
#    - Python-level caching for adapter API responses
#    - Independent of SearXNG's cache

# =================================
# SearXNG configuration (root-level options for SearXNG)
# =================================
use_default_settings: true

server:
  # base_url is taken from the SEARXNG_BASE_URL environment variable
  secret_key: "CHANGE_ME_TO_RANDOM_SECRET_KEY_32_CHARS_OR_MORE"  # MUST UPDATE
  limiter: false  # Disable rate limiting for API workloads
  image_proxy: true
  
# Output formats (JSON is required for the API)
search:
  formats:
    - html
    - json    # Required for the API
    - csv
    - rss

# =========================================================================
# Redis/Valkey settings (SearXNG built-in caching)
# =========================================================================
# SearXNG uses Redis/Valkey for:
# - Search results caching (automatic, no TTL configuration)
# - Rate limiting (when server.limiter is enabled)
# - Session management
# - Autocomplete caching
# 
# Docker network: redis://redis:6379/0
# Azure/localhost: redis://localhost:6379/0
valkey:
  url: redis://redis:6379/0

# Outgoing request tuning
outgoing:
  request_timeout: 10.0
  max_request_timeout: 15.0
  
# Search engine configuration
engines:
  - name: google
    disabled: false
  - name: bing
    disabled: true    # Disabled per request
  - name: duckduckgo
    disabled: false
  - name: brave
    disabled: false
  - name: startpage
    disabled: true    # Disabled due to stability issues
  - name: yacy images
    disabled: true    # Disabled due to stability issues

# =================================
# Tavily Adapter configuration
# =================================
adapter:
  # Adapter server options
  server:
    host: "0.0.0.0"
    port: 8000

  # URL used to reach SearXNG
  # Internal Docker URL (same network): "http://searxng:8080"
  # External URL (standalone SearXNG): "http://localhost:8999"
  searxng_url: "http://searxng:8080"

  # Scraper settings
  scraper:
    timeout: 30  # Increased from 10 to reduce timeout failures during raw content scraping
    max_content_length: 2500
    user_agent: "Mozilla/5.0 (compatible; TavilyBot/1.0)"  # Update if you need a custom agent

  # Default search behaviour
  search:
    default_max_results: 10
    default_engines: "google,duckduckgo,brave"
    default_categories: "general"
    default_language: "auto"
    safesearch: 1
    
    # =====================================================================
    # Adapter's own in-memory cache (separate from SearXNG's Redis cache)
    # =====================================================================
    # These settings control the adapter's Python-level caching layer:
    # - cache_ttl_seconds: How long adapter keeps cached search responses
    # - cache_max_entries: Maximum cached queries in adapter's memory
    # - response_cache_ttl_seconds: TTL for HTTP response cache
    # - response_cache_max_entries: Max response entries in adapter's memory
    # 
    # Note: This is independent of SearXNG's Redis/Valkey cache configured above
    cache_ttl_seconds: 120         # Adapter cache TTL (seconds)
    cache_max_entries: 256         # Max cached queries in adapter memory
    response_cache_ttl_seconds: 60  # HTTP response cache TTL (seconds)
    response_cache_max_entries: 128 # Max response cache entries in adapter memory

  # Tavily Extract API limits
  extract:
    max_urls: 20            # Maximum URLs per /extract request
    timeout_basic: 30       # Timeout (seconds) for basic mode - increased from 12
    timeout_advanced: 45    # Timeout (seconds) for advanced mode - increased from 25
    default_format: "markdown"
